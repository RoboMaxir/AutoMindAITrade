import numpy as np
import pandas as pd
from ta.trend import EMAIndicator
from ta.momentum import RSIIndicator
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
import pickle
import ccxt
import warnings
warnings.filterwarnings("ignore")

class PatternMemory:
    def __init__(self, name="trade_patterns_v1"):
        self.name = name
        self.patterns = []  # Ù„ÛŒØ³Øª Ø§Ù„Ú¯ÙˆÙ‡Ø§: {fingerprint, future_return, metadata}
        self.scaler = StandardScaler()
        self.nn = None  # Nearest Neighbors Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØªØ±ÛŒÙ† Ø§Ù„Ú¯Ùˆ

    # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø§Ø³ØªØ®Ø±Ø§Ø¬ "Ø³Ø±Ù†Ú¯Ø´Øª Ø§Ù„Ú¯Ùˆ" Ø§Ø² ÛŒÚ© Ù‚Ø·Ø¹Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±
    def encode_pattern(self, df_segment):
        """ØªØ¨Ø¯ÛŒÙ„ ÛŒÚ© ØªÚ©Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± (Ù…Ø«Ù„Ø§Ù‹ Û±Ûµ Ú©Ù†Ø¯Ù„) Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ø«Ø§Ø¨Øª (fingerprint)"""
        closes = df_segment['close'].values
        highs = df_segment['high'].values
        lows = df_segment['low'].values
        volumes = df_segment['volume'].values

        # Normalization Ù†Ø³Ø¨ÛŒ: ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§ÙˆÙ„ÛŒÙ† Ù‚ÛŒÙ…Øª
        base = closes[0]
        norm_close = (closes - base) / base
        norm_high = (highs - base) / base
        norm_low = (lows - base) / base
        norm_vol = volumes / volumes.mean() if volumes.mean() != 0 else np.ones_like(volumes)

        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:
        features = [
            # Ø´Ú©Ù„ Ú©Ù„ÛŒ
            norm_close[-1],                         # Ø¨Ø§Ø²Ú¯Ø´Øª/Ø§Ø¯Ø§Ù…Ù‡ØŸ
            np.max(norm_high),                      # Ø³Ù‚Ù Ù†Ø³Ø¨ÛŒ
            np.min(norm_low),                       # Ú©Ù Ù†Ø³Ø¨ÛŒ
            np.std(norm_close),                     # Ù†ÙˆØ³Ø§Ù†
            # RSI Ùˆ EMA Ø¯Ø§Ø®Ù„ÛŒ
            RSIIndicator(pd.Series(closes), 14).rsi().iloc[-1] / 100,  # Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡
            (EMAIndicator(pd.Series(closes), 5).ema_indicator().iloc[-1] - base) / base,
            (EMAIndicator(pd.Series(closes), 20).ema_indicator().iloc[-1] - base) / base,
            # Ø­Ø¬Ù…
            norm_vol[-1],
            np.mean(norm_vol[-5:]),                 # Ø­Ø¬Ù… Ûµ Ú©Ù†Ø¯Ù„ Ø§Ø®ÛŒØ±
        ]
        return np.array(features, dtype=np.float32)

    # Ù…Ø±Ø­Ù„Ù‡ Û²: Ø¢Ù…ÙˆØ²Ø´ Ø­Ø§ÙØ¸Ù‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
    def learn_from_history(self, df, window_size=15, future_horizon=5):
        """
        df: Ø¯Ø§Ø¯Ù‡ ØªØ§Ø±ÛŒØ®ÛŒ (OHLCV)
        window_size: Ø·ÙˆÙ„ Ø§Ù„Ú¯Ùˆ (Ù…Ø«Ù„Ø§Ù‹ Û±Ûµ Ú©Ù†Ø¯Ù„)
        future_horizon: Ú†Ù†Ø¯ Ú©Ù†Ø¯Ù„ Ø¬Ù„ÙˆØªØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ… (Ù…Ø«Ù„Ø§Ù‹ Ûµ Ú©Ù†Ø¯Ù„ = Û²Û° Ø³Ø§Ø¹Øª Ø¨Ø±Ø§ÛŒ 4h)
        """
        print(f"ğŸ§  Training Pattern Memory on {len(df)} candles...")
        for i in range(window_size, len(df) - future_horizon):
            segment = df.iloc[i - window_size:i]
            future_price = df.iloc[i + future_horizon]['close']
            current_price = df.iloc[i - 1]['close']
            future_return = (future_price - current_price) / current_price  # Ø¯Ø±ØµØ¯ ØªØºÛŒÛŒØ±

            fingerprint = self.encode_pattern(segment)
            self.patterns.append({
                'fingerprint': fingerprint,
                'future_return': future_return,
                'timestamp': df.iloc[i]['timestamp'],
                'symbol': getattr(df, 'symbol', 'N/A'),
                'avg_volume': segment['volume'].mean(),
                'volatility': segment['high'].max() - segment['low'].min()
            })

        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØªØ±ÛŒÙ† Ø§Ù„Ú¯Ùˆ
        X = np.array([p['fingerprint'] for p in self.patterns])
        self.scaler.fit(X)
        X_scaled = self.scaler.transform(X)
        self.nn = NearestNeighbors(n_neighbors=5, metric='cosine').fit(X_scaled)
        print(f"âœ… Pattern Memory trained on {len(self.patterns)} historical patterns.")

    # Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾Ø±Ø³â€ŒÙˆØ¬Ùˆ â€” "Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆÛŒ Ø¬Ø¯ÛŒØ¯ Ø´Ø¨ÛŒÙ‡ Ú†Ù‡ Ú†ÛŒØ²Ù‡Ø§ÛŒÛŒ ØªÙˆÛŒ Ø­Ø§ÙØ¸Ù…Ù‡ØŸ"
    def query(self, current_segment):
        fp = self.encode_pattern(current_segment).reshape(1, -1)
        fp_scaled = self.scaler.transform(fp)
        distances, indices = self.nn.kneighbors(fp_scaled)

        similar = []
        for dist, idx in zip(distances[0], indices[0]):
            pat = self.patterns[idx]
            similar.append({
                'similarity': 1 - dist,  # Ù‡Ø±Ú†Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ø¨Ù‡ Û±ØŒ Ø´Ø¨ÛŒÙ‡â€ŒØªØ±
                'future_return': pat['future_return'],
                'timestamp': pat['timestamp'],
                'symbol': pat['symbol']
            })
        return similar

    # Ø°Ø®ÛŒØ±Ù‡/Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡
    def save(self, path=None):
        path = path or f"{self.name}.pkl"
        with open(path, 'wb') as f:
            pickle.dump({
                'patterns': self.patterns,
                'scaler': self.scaler,
                'nn': self.nn
            }, f)
        print(f"ğŸ’¾ Pattern Memory saved to {path}")

    def load(self, path=None):
        path = path or f"{self.name}.pkl"
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
                self.patterns = data['patterns']
                self.scaler = data['scaler']
                self.nn = data['nn']
            print(f"ğŸ“‚ Pattern Memory loaded from {path} ({len(self.patterns)} patterns)")
            return True
        except:
            print("âš ï¸ No saved memory found. Training from scratch.")
            return False